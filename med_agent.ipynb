{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "986f6fda-6882-4340-902d-9839285d92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "# Allows nested asyncio event loops - Jupyter already runs in an event loop,\n",
    "# so nest_asyncio lets us use asyncio.run() within this environment.\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0bd951-2cbd-4519-a129-050637ffab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.agents import Agent\n",
    "from haystack_integrations.components.generators.ollama import OllamaChatGenerator\n",
    "from haystack.components.tools import ToolInvoker\n",
    "from haystack.components.converters import OutputAdapter\n",
    "from haystack.dataclasses import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4beca971-5ad1-4a81-8cc4-e2d3510eab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = OllamaChatGenerator(model=\"qwen2.5:32b\", timeout=90, url=\"http://localhost:11434\", streaming_callback=lambda chunk: print(chunk.content, end=\"\", flush=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713cc968-cb4f-435d-8744-39926c12f473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing (NLP) is a branch of Artificial Intelligence that focuses on enabling computers to understand, interpret, and generate human languages in a valuable way. It involves the development of algorithms and models that can process large amounts of natural language data to perform tasks such as text classification, sentiment analysis, machine translation, speech recognition, and more.\n",
      "\n",
      "To get started with NLP, you can follow these steps:\n",
      "\n",
      "1. **Learn the Basics**: Start by understanding the fundamental concepts in linguistics and computer science relevant to NLP. This"
     ]
    }
   ],
   "source": [
    "# messages = [\n",
    "#     ChatMessage.from_user(\"What's Natural Language Processing?\"),\n",
    "#     ChatMessage.from_system(\n",
    "#         \"Natural Language Processing (NLP) is a field of computer science and artificial \"\n",
    "#         \"intelligence concerned with the interaction between computers and human language\"\n",
    "#     ),\n",
    "#     ChatMessage.from_user(\"How do I get started?\"),\n",
    "# ]\n",
    "\n",
    "# response = llm.run(messages, generation_kwargs={\"temperature\": 0.2})\n",
    "\n",
    "# print(response[\"replies\"][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0610b951-66c4-4ba1-bccf-8a3c36f4d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_integrations.tools.mcp import MCPToolset, StdioServerInfo, SSEServerInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc4960de-6212-41e2-859b-89032a368295",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_info = StdioServerInfo(command=\"uv\", args=[\"run\", \"--with\", \"biomcp-python\", \"biomcp\", \"run\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c20d7597-a77a-466a-83e8-d60f464b4d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsafe mode is enabled. This allows execution of arbitrary code in the Jinja template. Use this only if you trust the source of the template.\n"
     ]
    },
    {
     "ename": "PipelineRuntimeError",
     "evalue": "The following component failed to run:\nComponent name: 'llm'\nComponent type: 'OllamaChatGenerator'\nError: Ollama does not support tools and streaming at the same time. Please choose one.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/med-oryx/.venv/lib/python3.11/site-packages/haystack/core/pipeline/pipeline.py:80\u001b[39m, in \u001b[36mPipeline._run_component\u001b[39m\u001b[34m(self, component, inputs, component_visits, parent_span)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     component_output = \u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/med-oryx/.venv/lib/python3.11/site-packages/haystack_integrations/components/generators/ollama/chat/chat_generator.py:314\u001b[39m, in \u001b[36mOllamaChatGenerator.run\u001b[39m\u001b[34m(self, messages, generation_kwargs, tools, streaming_callback)\u001b[39m\n\u001b[32m    313\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mOllama does not support tools and streaming at the same time. Please choose one.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_format \u001b[38;5;129;01mand\u001b[39;00m tools:\n",
      "\u001b[31mValueError\u001b[39m: Ollama does not support tools and streaming at the same time. Please choose one.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mPipelineRuntimeError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m user_input = \u001b[33m\"\u001b[39m\u001b[33mCreate a short report on clinical trails with Diclofenac.\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# can be any city\u001b[39;00m\n\u001b[32m     21\u001b[39m user_input_msg = ChatMessage.from_user(text=user_input)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m result = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_input_msg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madapter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minitial_msg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_input_msg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(result[\u001b[33m\"\u001b[39m\u001b[33mresponse_llm\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/med-oryx/.venv/lib/python3.11/site-packages/haystack/core/pipeline/pipeline.py:247\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, data, include_outputs_from)\u001b[39m\n\u001b[32m    242\u001b[39m     cached_topological_sort = topological_sort\n\u001b[32m    243\u001b[39m     component = \u001b[38;5;28mself\u001b[39m._get_component_with_graph_metadata_and_visits(\n\u001b[32m    244\u001b[39m         component_name, component_visits[component_name]\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m component_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_component\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent_visits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_span\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# Updates global input state with component outputs and returns outputs that should go to\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# pipeline outputs.\u001b[39;00m\n\u001b[32m    252\u001b[39m component_pipeline_outputs = \u001b[38;5;28mself\u001b[39m._write_component_outputs(\n\u001b[32m    253\u001b[39m     component_name=component_name,\n\u001b[32m    254\u001b[39m     component_outputs=component_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m    257\u001b[39m     include_outputs_from=include_outputs_from,\n\u001b[32m    258\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/med-oryx/.venv/lib/python3.11/site-packages/haystack/core/pipeline/pipeline.py:82\u001b[39m, in \u001b[36mPipeline._run_component\u001b[39m\u001b[34m(self, component, inputs, component_visits, parent_span)\u001b[39m\n\u001b[32m     80\u001b[39m     component_output = instance.run(**component_inputs)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineRuntimeError.from_exception(component_name, instance.\u001b[34m__class__\u001b[39m, error) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m\n\u001b[32m     83\u001b[39m component_visits[component_name] += \u001b[32m1\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(component_output, Mapping):\n",
      "\u001b[31mPipelineRuntimeError\u001b[39m: The following component failed to run:\nComponent name: 'llm'\nComponent type: 'OllamaChatGenerator'\nError: Ollama does not support tools and streaming at the same time. Please choose one."
     ]
    }
   ],
   "source": [
    "# Create the toolset - this will automatically discover all available tools\n",
    "mcp_toolset = MCPToolset(server_info)\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\"llm\",  OllamaChatGenerator(model=\"mistral-small3.1\", timeout=300, url=\"http://localhost:11434\", tools=mcp_toolset, streaming_callback=lambda chunk: print(chunk.content, end=\"\", flush=True)))\n",
    "pipeline.add_component(\"tool_invoker\", ToolInvoker(tools=mcp_toolset))\n",
    "pipeline.add_component(\n",
    "    \"adapter\",\n",
    "    OutputAdapter(\n",
    "        template=\"{{ initial_msg + initial_tool_messages + tool_messages }}\",\n",
    "        output_type=list[ChatMessage],\n",
    "        unsafe=True,\n",
    "    ),\n",
    ")\n",
    "pipeline.add_component(\"response_llm\",  OllamaChatGenerator(model=\"mistral-small3.1\", timeout=300, url=\"http://localhost:11434\", streaming_callback=lambda chunk: print(chunk.content, end=\"\", flush=True)))\n",
    "pipeline.connect(\"llm.replies\", \"tool_invoker.messages\")\n",
    "pipeline.connect(\"llm.replies\", \"adapter.initial_tool_messages\")\n",
    "pipeline.connect(\"tool_invoker.tool_messages\", \"adapter.tool_messages\")\n",
    "pipeline.connect(\"adapter.output\", \"response_llm.messages\")\n",
    "\n",
    "user_input = \"Create a short report on clinical trails with Diclofenac.\"  # can be any city\n",
    "user_input_msg = ChatMessage.from_user(text=user_input)\n",
    "\n",
    "result = pipeline.run({\"llm\": {\"messages\": [user_input_msg]}, \"adapter\": {\"initial_msg\": [user_input_msg]}})\n",
    "\n",
    "print(result[\"response_llm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c494b1c-6dde-4fa7-8c34-6498797277dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03ea332-97ea-4e0a-813b-d24f9df85cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
